{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2836be02-1cfc-4b12-bf54-13dcefd6ce03",
   "metadata": {},
   "source": [
    "# Finance #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd1f31-1462-4466-b09e-947f5e22bf26",
   "metadata": {},
   "source": [
    "## Load and initialize data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69ddd14-3ff6-433b-a8da-926588911a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- sched_dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- sched_arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- time_hour: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- average_ticket_price1: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- fuel_cost: double (nullable = true)\n",
      " |-- average_revenue: double (nullable = true)\n",
      " |-- flight_profit: double (nullable = true)\n",
      " |-- time_of_day: integer (nullable = true)\n",
      " |-- average_ticket_price7: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Data Preprocessing + Finance Pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "#  Google Storage File Path\n",
    "delay_path = 'gs://de_as2_data/flights_delaydata.csv' \n",
    "time_path = 'gs://de_as2_data/flights_timedata.csv' \n",
    "info_path = 'gs://de_as2_data/flights_infodata.csv'\n",
    "finance_path = 'gs://de_as2_data/flight_financedata.csv'\n",
    "# Create data frame\n",
    "df_delay = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(delay_path)\n",
    "df_delay.printSchema()\n",
    "\n",
    "df_time = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(time_path)\n",
    "df_time.printSchema()\n",
    "\n",
    "df_info = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(info_path)\n",
    "df_info.printSchema()\n",
    "\n",
    "df_finance = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(finance_path)\n",
    "df_finance.printSchema()\n",
    "\n",
    "\n",
    "# Inferschema makes all data types adjust dynamically\n",
    "# APPOINT WORKER TO EACH PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef033e-7b27-4dcc-975b-012ba28661c7",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n",
    "In the next section, we will perform data cleaning techniques, such as removing rows with missing values, removing duplicates and removing extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ab57cf-2d0b-4cc7-8738-7db94a718b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count flights_delaydata: 336776\n",
      "Original row count flights_timedata: 336776\n",
      "Original row count flights_infodata: 336776\n",
      "Original row count flights_financedata: 336776\n"
     ]
    }
   ],
   "source": [
    "# Show original data count\n",
    "print(\"Original row count flights_delaydata:\", df_delay.count())\n",
    "print(\"Original row count flights_timedata:\", df_time.count())\n",
    "print(\"Original row count flights_infodata:\", df_info.count())\n",
    "print(\"Original row count flights_financedata:\", df_finance.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e511f-79cf-4e9b-9d23-72773c987faa",
   "metadata": {},
   "source": [
    "We drop rows with missing values to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07267125-a574-4d21-b637-aaf47c954a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values\n",
    "df_delay = df_delay.dropna()\n",
    "df_time = df_time.dropna()\n",
    "df_info = df_info.dropna()\n",
    "df_finance = df_finance.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7bd25-11ec-47dc-adfb-bc916360f349",
   "metadata": {},
   "source": [
    "Next, we remove rows containing duplicate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d9d20f6-d0c8-49ec-8a5f-86ecb4207cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows containing duplicate information\n",
    "df_delay = df_delay.dropDuplicates()\n",
    "df_time = df_time.dropDuplicates()\n",
    "df_info = df_info.dropDuplicates()\n",
    "df_finance = df_finance.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "304c4e86-c70a-4275-b205-584177d8eb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----------+---------------+-------------+-----------+--------------------+\n",
      "|    id|passenger_count|  fuel_cost|average_revenue|flight_profit|time_of_day|average_ticket_price|\n",
      "+------+---------------+-----------+---------------+-------------+-----------+--------------------+\n",
      "|235657|            268|2141.746343|     50329.7516|  48188.00526|          9|         263.1536318|\n",
      "+------+---------------+-----------+---------------+-------------+-----------+--------------------+\n",
      "\n",
      "+------+---------------+-----------+---------------+-------------+-----------+--------------------+\n",
      "|    id|passenger_count|  fuel_cost|average_revenue|flight_profit|time_of_day|average_ticket_price|\n",
      "+------+---------------+-----------+---------------+-------------+-----------+--------------------+\n",
      "|235657|            268|2141.746343|     50329.7516|  48188.00526|          9|         263.1536318|\n",
      "+------+---------------+-----------+---------------+-------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_finance.show(10)\n",
    "df_finance.filter(df_finance.id == '235657 ').show()\n",
    "df_finance = df_finance.drop(\"average_ticket_price1\")\n",
    "df_finance= df_finance.withColumnRenamed(\"average_ticket_price7\", \"average_ticket_price\")\n",
    "#df_finance.show(10)\n",
    "df_finance.filter(df_finance.id == '235657 ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f3c19-519d-434c-80fc-f281f0f06420",
   "metadata": {},
   "source": [
    "Lastly, we remove extreme outliers using the Interquartile Range (IQR) statistical method, as it is robust at detecting outliers. This is only neccessary for the attribute 'arr_delay' in flights_delaydata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478e4935-0329-4519-957b-e94813c88f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Remove extreme outliers in average_ticket_price in financedata\n",
    "column_price = \"average_ticket_price\"\n",
    "\n",
    "q1, q3 = df_finance.approxQuantile(column_price, [0.25, 0.75], 0.01)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate lower- and upper bound respectively\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "df_finance = df_finance.filter((col(column_price) >= lower_bound) & (col(column_price) <= upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf876083-d9af-4f6f-9ba0-c55ffee2af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned row count flights_delaydata: 327346\n",
      "Cleaned row count flights_timedata: 327346\n",
      "Cleaned row count flights_infodata: 334264\n",
      "Cleaned row count flights_financedata: 334526\n"
     ]
    }
   ],
   "source": [
    "# Show cleaned data count\n",
    "print(\"Cleaned row count flights_delaydata:\", df_delay.count())\n",
    "print(\"Cleaned row count flights_timedata:\", df_time.count())\n",
    "print(\"Cleaned row count flights_infodata:\", df_info.count())\n",
    "print(\"Cleaned row count flights_financedata:\", df_finance.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4bb5b0-4bc6-480e-95e8-ce1f19a97292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|average_ticket_price|\n",
      "+-------+--------------------+\n",
      "|  count|              334526|\n",
      "|   mean|  301.17452448213646|\n",
      "| stddev|  130.63841893106007|\n",
      "|    min|         80.40255821|\n",
      "|    max|         682.3777921|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_finance.select('average_ticket_price').describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65565fa4-ec59-40fc-b8b1-1811514708b6",
   "metadata": {},
   "source": [
    "## New variable creation: time_of_day ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b7939-db1e-4c3a-8b06-a6bc3def83c1",
   "metadata": {},
   "source": [
    "Convert time_hour to timestamp format, so that it can be used during logic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48252487-5254-4ec0-a33f-97b92ebccd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "| time_hour_datetime|time_of_day_dep|\n",
      "+-------------------+---------------+\n",
      "|2013-01-01 15:00:00|      Afternoon|\n",
      "|2013-01-02 06:00:00|        Morning|\n",
      "|2013-01-02 07:00:00|            Day|\n",
      "|2013-01-02 09:00:00|            Day|\n",
      "|2013-01-02 12:00:00|            Day|\n",
      "|2013-01-02 14:00:00|      Afternoon|\n",
      "|2013-01-02 16:00:00|      Afternoon|\n",
      "|2013-01-02 18:00:00|      Afternoon|\n",
      "|2013-01-02 19:00:00|          Night|\n",
      "|2013-01-03 08:00:00|            Day|\n",
      "|2013-01-04 08:00:00|            Day|\n",
      "|2013-01-04 10:00:00|            Day|\n",
      "|2013-01-04 08:00:00|            Day|\n",
      "|2013-01-05 05:00:00|        Morning|\n",
      "|2013-01-05 06:00:00|        Morning|\n",
      "|2013-01-05 09:00:00|            Day|\n",
      "|2013-01-05 12:00:00|            Day|\n",
      "|2013-01-05 15:00:00|      Afternoon|\n",
      "|2013-01-05 18:00:00|      Afternoon|\n",
      "|2013-01-06 07:00:00|            Day|\n",
      "+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- time_hour: string (nullable = true)\n",
      " |-- time_hour_datetime: timestamp (nullable = true)\n",
      " |-- time_of_day_dep: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, hour, when\n",
    "\n",
    "\n",
    "# Convert 'time_hour' to datetime type\n",
    "df_time_new = df_time.withColumn(\"time_hour_datetime\", to_timestamp(col(\"time_hour\"), \"dd/MM/yyyy HH:mm\"))\n",
    "\n",
    "# Add new categorical column\n",
    "df_time_new = df_time_new.withColumn(\n",
    "    \"time_of_day_dep\",\n",
    "    when((hour(col(\"time_hour_datetime\")) >= 0) & (hour(col(\"time_hour_datetime\")) <= 6), \"Morning\")\n",
    "    .when((hour(col(\"time_hour_datetime\")) >= 7) & (hour(col(\"time_hour_datetime\")) <= 12), \"Day\")\n",
    "    .when((hour(col(\"time_hour_datetime\")) >= 13) & (hour(col(\"time_hour_datetime\")) <= 18), \"Afternoon\")\n",
    "    .otherwise(\"Night\")  \n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_time_new.select(\"time_hour_datetime\", \"time_of_day_dep\").show()\n",
    "\n",
    "df_time_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d45c9f-9d26-4939-9990-4d9fe5439654",
   "metadata": {},
   "source": [
    "Drop time_hour as it is redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91008a9-1ed8-41e5-8904-67947b6c6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+----+------+-------------------+---------------+\n",
      "|  id|year|month|day|air_time|hour|minute| time_hour_datetime|time_of_day_dep|\n",
      "+----+----+-----+---+--------+----+------+-------------------+---------------+\n",
      "| 473|2013|    1|  1|     163|  15|    30|2013-01-01 15:00:00|      Afternoon|\n",
      "| 874|2013|    1|  2|     167|   6|    10|2013-01-02 06:00:00|        Morning|\n",
      "|1022|2013|    1|  2|     198|   7|    51|2013-01-02 07:00:00|            Day|\n",
      "|1072|2013|    1|  2|     160|   9|     4|2013-01-02 09:00:00|            Day|\n",
      "|1294|2013|    1|  2|     159|  12|    40|2013-01-02 12:00:00|            Day|\n",
      "|1318|2013|    1|  2|     154|  14|     0|2013-01-02 14:00:00|      Afternoon|\n",
      "|1453|2013|    1|  2|     166|  16|     5|2013-01-02 16:00:00|      Afternoon|\n",
      "|1630|2013|    1|  2|     238|  18|    15|2013-01-02 18:00:00|      Afternoon|\n",
      "|1711|2013|    1|  2|      53|  19|    42|2013-01-02 19:00:00|          Night|\n",
      "|1983|2013|    1|  3|      97|   8|    55|2013-01-03 08:00:00|            Day|\n",
      "+----+----+-----+---+--------+----+------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_new = df_time_new.drop(\"time_hour\")\n",
    "df_time_new.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bf300-dae8-4a7f-bcf6-8635331a8b83",
   "metadata": {},
   "source": [
    "## Join Dataframes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daca1498-8980-46c3-b95e-475f0e054fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------+---------------+-------------+--------------------+-------+------+-------+------+----+--------+--------------------+----+-----+---+--------+----+------+----------------+\n",
      "| id|passenger_count|  fuel_cost|average_revenue|flight_profit|average_ticket_price|carrier|flight|tailnum|origin|dest|distance|                name|year|month|day|air_time|hour|minute|       time_hour|\n",
      "+---+---------------+-----------+---------------+-------------+--------------------+-------+------+-------+------+----+--------+--------------------+----+-----+---+--------+----+------+----------------+\n",
      "|  1|            205|1032.268511|    98458.57313|  97426.30461|         622.6722533|     UA|  1714| N24211|   LGA| IAH|    1416|United Air Lines ...|2013|    1|  1|     227|   5|    29|01/01/2013 05:00|\n",
      "|  3|            220|4325.253537|    74681.94661|  70356.69307|         293.0288722|     B6|   725| N804JB|   JFK| BQN|    1576|     JetBlue Airways|2013|    1|  1|     183|   5|    45|01/01/2013 05:00|\n",
      "|  5|             65|2743.406405|    10555.85753|  7812.451124|          138.379599|     UA|  1696| N39463|   EWR| ORD|     719|United Air Lines ...|2013|    1|  1|     150|   5|    58|01/01/2013 05:00|\n",
      "|  6|             52|2066.959006|    6408.139133|  4341.180127|         105.3698231|     B6|   507| N516JB|   EWR| FLL|    1065|     JetBlue Airways|2013|    1|  1|     158|   6|     0|01/01/2013 06:00|\n",
      "|  9|            263|3232.784867|    100789.2352|  97556.45032|         361.3273127|     AA|   301| N3ALAA|   LGA| ORD|     733|American Airlines...|2013|    1|  1|     138|   6|     0|01/01/2013 06:00|\n",
      "| 12|            286|2701.834119|    123831.4381|   121129.604|         355.4015984|     UA|   194| N29129|   JFK| LAX|    2475|United Air Lines ...|2013|    1|  1|     345|   6|     0|01/01/2013 06:00|\n",
      "| 13|            134|1291.422546|    24781.37633|  23489.95379|         154.1222043|     UA|  1124| N53441|   EWR| SFO|    2565|United Air Lines ...|2013|    1|  1|     361|   6|     0|01/01/2013 06:00|\n",
      "| 15|             60|4136.111466|    10401.70824|   6265.59677|         168.8079247|     B6|  1806| N708JB|   JFK| BOS|     187|     JetBlue Airways|2013|    1|  1|      44|   5|    59|01/01/2013 05:00|\n",
      "| 16|            283|3198.314498|     62740.2219|  59541.90741|         200.1860618|     UA|  1187| N76515|   EWR| LAS|    2227|United Air Lines ...|2013|    1|  1|     337|   6|     0|01/01/2013 06:00|\n",
      "| 17|            255|2053.678227|    79025.15603|   76971.4778|         290.3530511|     B6|   371| N595JB|   LGA| FLL|    1076|     JetBlue Airways|2013|    1|  1|     152|   6|     0|01/01/2013 06:00|\n",
      "+---+---------------+-----------+---------------+-------------+--------------------+-------+------+-------+------+----+--------+--------------------+----+-----+---+--------+----+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df_finance\n",
    "df2 = df_info\n",
    "df3 = df_time_new\n",
    "\n",
    "joined_df_finance = df1.join(df2, on=\"id\", how=\"inner\") \\\n",
    "                    .join(df3, on=\"id\", how=\"inner\") \\\n",
    "\n",
    "joined_df_finance = joined_df_finance.drop(\"time_of_day\")\n",
    "# Save the final DataFrame to GCS as a CSV\n",
    "joined_df_finance.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee5b55-d6e1-4e20-b4a3-cbd044253a18",
   "metadata": {},
   "source": [
    "# Upload new Dataframe to bucket #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3366ce5a-cb66-4651-a705-eeeb18bd4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df_finance.write.mode(\"overwrite\").csv(\"gs://de_as2_data/joined_flights_financedata_final\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ad1eb-2455-4963-aaf6-ca71302a1430",
   "metadata": {},
   "source": [
    "## Upload to BigQuery ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9213fd-755c-473e-9a46-4a6f49940ca7",
   "metadata": {},
   "source": [
    "Ensure all neccessary dependencies are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58d60b34-53be-452d-bcc4-8a8cd55ae38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.11/site-packages (3.27.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.11.1 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (2.23.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.36.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.7.2)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (23.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (5.28.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.25.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.68.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.68.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-resumable-media<3.0dev,>=2.0.0->google-cloud-bigquery) (1.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbac639d-6cef-4361-b87d-2cb6ec93b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully uploaded to BigQuery, and the table was overwritten.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=\"chromatic-pride-435508-i5\")   \n",
    "\n",
    "# Define dataset and table\n",
    "dataset_id = \"chromatic-pride-435508-i5.flights\"\n",
    "table_id = f\"{dataset_id}.Finance_Pipeline\"\n",
    "\n",
    "# Configure the load job\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1  # Ignore the header row\n",
    "job_config.autodetect = True\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # Overwrite the table\n",
    "\n",
    "# Path to the data in Google Cloud Storage\n",
    "gcs_path = \"gs://de_as2_data/joined_flights_financedata_final/*.csv\"  # Combines all matching CSV files\n",
    "\n",
    "# Start the load job\n",
    "load_job = client.load_table_from_uri(gcs_path, table_id, job_config=job_config)\n",
    "\n",
    "# Wait for the job to complete\n",
    "load_job.result()\n",
    "\n",
    "print(\"Data successfully uploaded to BigQuery, and the table was overwritten.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb7696-0942-4156-8eb0-9594c0cadd47",
   "metadata": {},
   "source": [
    "### Stop spark session ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3c8490a2-dcc8-40ef-9b05-9abefa76feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99157f3-a531-4f01-840c-1cbe146989e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
