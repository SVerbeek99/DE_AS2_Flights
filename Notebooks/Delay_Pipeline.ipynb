{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1de75b-23ec-43e9-9ebb-ad373e933efc",
   "metadata": {},
   "source": [
    "# Data Preprocessing #\n",
    "In the next section, we will perform data cleaning techniques, such as removing rows with missing values, removing duplicates and removing extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1690f074-c80c-40a3-8d5f-e1c3f4e310c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- sched_dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- sched_arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- time_hour: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Data Preprocessing + Delay Pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "#  Google Storage File Path\n",
    "delay_path = 'gs://de_as2_data/flights_delaydata.csv' \n",
    "time_path = 'gs://de_as2_data/flights_timedata.csv' \n",
    "info_path = 'gs://de_as2_data/flights_infodata.csv'\n",
    "# Create data frame\n",
    "df_delay = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(delay_path)\n",
    "df_delay.printSchema()\n",
    "\n",
    "df_time = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(time_path)\n",
    "df_time.printSchema()\n",
    "\n",
    "df_info = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(info_path)\n",
    "df_info.printSchema()\n",
    "\n",
    "# Inferschema makes all data types adjust dynamically\n",
    "# APPOINT WORKER TO EACH PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd71ee71-0d66-4c5e-a94f-7937b235d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count flights_delaydata: 336776\n",
      "Original row count flights_timedata: 336776\n",
      "Original row count flights_infodata: 336776\n"
     ]
    }
   ],
   "source": [
    "# Show original data count\n",
    "print(\"Original row count flights_delaydata:\", df_delay.count())\n",
    "print(\"Original row count flights_timedata:\", df_time.count())\n",
    "print(\"Original row count flights_infodata:\", df_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a93219-485d-43af-b4cc-e1e1506350b7",
   "metadata": {},
   "source": [
    "We drop rows with missing values to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d94d90-e0ef-4f76-8609-c979cde7a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values\n",
    "df_delay = df_delay.dropna()\n",
    "df_time = df_time.dropna()\n",
    "df_info = df_info.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546711c7-3332-48db-9fd3-498bf3a26061",
   "metadata": {},
   "source": [
    "Next, we remove rows containing duplicate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461ceeec-e1d2-4e31-b2be-ab54e1e02a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows containing duplicate information\n",
    "df_delay = df_delay.dropDuplicates()\n",
    "df_time = df_time.dropDuplicates()\n",
    "df_info = df_info.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9ec39-b698-4847-bb81-5c6d9b243e95",
   "metadata": {},
   "source": [
    "Lastly, we remove extreme outliers using the Interquartile Range (IQR) statistical method, as it is robust at detecting outliers. This is only neccessary for the attribute 'arr_delay' in flights_delaydata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f30c8c-98ee-44f2-afca-97d49d4ba38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove extreme outliers in arr_delay column in delaydata\n",
    "column = \"arr_delay\"\n",
    "q1, q3 = df_delay.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate lower- and upper bound respectively\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "df_delay = df_delay.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa1416a-8f3f-4a72-825b-fa611a2fc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned row count flights_delaydata: 299466\n",
      "Cleaned row count flights_timedata: 327346\n",
      "Cleaned row count flights_infodata: 334264\n"
     ]
    }
   ],
   "source": [
    "# Show cleaned data count\n",
    "print(\"Cleaned row count flights_delaydata:\", df_delay.count())\n",
    "print(\"Cleaned row count flights_timedata:\", df_time.count())\n",
    "print(\"Cleaned row count flights_infodata:\", df_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94df2c-e891-4606-89be-6b0a8e93721a",
   "metadata": {},
   "source": [
    "# New Variable Creation: delay_bin #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458337a-01b9-4683-87ca-6dfbbb8953f7",
   "metadata": {},
   "source": [
    "Create new categorical variable 'arr_delay_category', where if arr_delay is < 0 then 0 (early), if arr_delay = 0 (on time) then 1 and if arr_delay > 0 (late) then 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f767723-2d11-4525-b356-59214c5652d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|arr_delay|     delay_bin|\n",
      "+---------+--------------+\n",
      "|      -28|         Early|\n",
      "|      -13|         Early|\n",
      "|      -11|         Early|\n",
      "|       -6|         Early|\n",
      "|       43|Moderate Delay|\n",
      "|      -10|         Early|\n",
      "|      -15|         Early|\n",
      "|        3|       On Time|\n",
      "|      -10|         Early|\n",
      "|       -4|         Early|\n",
      "|       22|Moderate Delay|\n",
      "|      -20|         Early|\n",
      "|      -53|         Early|\n",
      "|      -17|         Early|\n",
      "|      -10|         Early|\n",
      "|       -6|         Early|\n",
      "|      -30|         Early|\n",
      "|       27|Moderate Delay|\n",
      "|       31|Moderate Delay|\n",
      "|      -17|         Early|\n",
      "|       12|       On Time|\n",
      "|       11|       On Time|\n",
      "|      -24|         Early|\n",
      "|       47|Moderate Delay|\n",
      "|        1|       On Time|\n",
      "|       21|Moderate Delay|\n",
      "|        6|       On Time|\n",
      "|      -14|         Early|\n",
      "|      -22|         Early|\n",
      "|      -32|         Early|\n",
      "|       -2|         Early|\n",
      "|      -36|         Early|\n",
      "|      -16|         Early|\n",
      "|        8|       On Time|\n",
      "|       42|Moderate Delay|\n",
      "|       59|Moderate Delay|\n",
      "|       -2|         Early|\n",
      "|       -7|         Early|\n",
      "|      -19|         Early|\n",
      "|        7|       On Time|\n",
      "|       -9|         Early|\n",
      "|       20|Moderate Delay|\n",
      "|       -6|         Early|\n",
      "|       -5|         Early|\n",
      "|       18|Moderate Delay|\n",
      "|      -28|         Early|\n",
      "|        4|       On Time|\n",
      "|       -8|         Early|\n",
      "|       57|Moderate Delay|\n",
      "|        1|       On Time|\n",
      "+---------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create a new column 'delay_bin' with bins based on arr_delay\n",
    "df_delay_new = df_delay.withColumn(\n",
    "    \"delay_bin\",\n",
    "    when(col(\"arr_delay\") < 0, \"Early\")\n",
    "    .when((col(\"arr_delay\") >= 0) & (col(\"arr_delay\") <= 15), \"On Time\")\n",
    "    .when((col(\"arr_delay\") > 15) & (col(\"arr_delay\") <= 60), \"Moderate Delay\")\n",
    "    .otherwise(\"Severe Delay\")  # arr_delay > 60\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_delay_new.select(\"arr_delay\", \"delay_bin\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1fa9a-e014-47d5-82f7-8b80cfcf8464",
   "metadata": {},
   "source": [
    "# Join Dataframes #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f4c174-c7d7-4184-9d73-9eaa943a9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df1, df2, and df3 are your three DataFrames\n",
    "df1 = df_delay_new\n",
    "df2 = df_info\n",
    "df3 = df_time\n",
    "\n",
    "joined_df_delay = df1.join(df2, on=\"id\", how=\"inner\") \\\n",
    "                    .join(df3, on=\"id\", how=\"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1fea485-2554-4ee0-be26-a4cff4512796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+---------+--------+--------------+---------+--------------+-------+------+-------+------+----+--------+--------------------+----+-----+---+--------+----+------+----------------+\n",
      "| id|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|     delay_bin|carrier|flight|tailnum|origin|dest|distance|                name|year|month|day|air_time|hour|minute|       time_hour|\n",
      "+---+--------+--------------+---------+--------+--------------+---------+--------------+-------+------+-------+------+----+--------+--------------------+----+-----+---+--------+----+------+----------------+\n",
      "|  1|     533|           529|        4|     850|           830|       20|Moderate Delay|     UA|  1714| N24211|   LGA| IAH|    1416|United Air Lines ...|2013|    1|  1|     227|   5|    29|01/01/2013 05:00|\n",
      "|  3|     544|           545|       -1|    1004|          1022|      -18|         Early|     B6|   725| N804JB|   JFK| BQN|    1576|     JetBlue Airways|2013|    1|  1|     183|   5|    45|01/01/2013 05:00|\n",
      "|  5|     554|           558|       -4|     740|           728|       12|       On Time|     UA|  1696| N39463|   EWR| ORD|     719|United Air Lines ...|2013|    1|  1|     150|   5|    58|01/01/2013 05:00|\n",
      "|  6|     555|           600|       -5|     913|           854|       19|Moderate Delay|     B6|   507| N516JB|   EWR| FLL|    1065|     JetBlue Airways|2013|    1|  1|     158|   6|     0|01/01/2013 06:00|\n",
      "|  9|     558|           600|       -2|     753|           745|        8|       On Time|     AA|   301| N3ALAA|   LGA| ORD|     733|American Airlines...|2013|    1|  1|     138|   6|     0|01/01/2013 06:00|\n",
      "| 12|     558|           600|       -2|     924|           917|        7|       On Time|     UA|   194| N29129|   JFK| LAX|    2475|United Air Lines ...|2013|    1|  1|     345|   6|     0|01/01/2013 06:00|\n",
      "| 13|     558|           600|       -2|     923|           937|      -14|         Early|     UA|  1124| N53441|   EWR| SFO|    2565|United Air Lines ...|2013|    1|  1|     361|   6|     0|01/01/2013 06:00|\n",
      "| 15|     559|           559|        0|     702|           706|       -4|         Early|     B6|  1806| N708JB|   JFK| BOS|     187|     JetBlue Airways|2013|    1|  1|      44|   5|    59|01/01/2013 05:00|\n",
      "| 16|     559|           600|       -1|     854|           902|       -8|         Early|     UA|  1187| N76515|   EWR| LAS|    2227|United Air Lines ...|2013|    1|  1|     337|   6|     0|01/01/2013 06:00|\n",
      "| 17|     600|           600|        0|     851|           858|       -7|         Early|     B6|   371| N595JB|   LGA| FLL|    1076|     JetBlue Airways|2013|    1|  1|     152|   6|     0|01/01/2013 06:00|\n",
      "+---+--------+--------------+---------+--------+--------------+---------+--------------+-------+------+-------+------+----+--------+--------------------+----+-----+---+--------+----+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df_delay.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83faca5f-8aa7-4160-9449-1cc1b8a8c640",
   "metadata": {},
   "source": [
    "# Upload new Dataframe to bucket #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "813d75bd-a774-407c-adc8-80dbd18aff5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path gs://de_as2_data/joined_flights_delaydata_final already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjoined_df_delay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://de_as2_data/joined_flights_delaydata_final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path gs://de_as2_data/joined_flights_delaydata_final already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "joined_df_delay.write.csv(\"gs://de_as2_data/joined_flights_delaydata_final\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab4d54-87ab-42c5-ab07-2a5ffc39daa7",
   "metadata": {},
   "source": [
    "# Upload to BigQuery #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f4cb61-5fc9-4b26-ab21-a0c3cf35d0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.11.1 (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth<3.0.0dev,>=2.14.1 (from google-cloud-bigquery)\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (23.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (4.24.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading grpcio-1.68.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-resumable-media<3.0dev,>=2.0.0->google-cloud-bigquery)\n",
      "  Downloading google_crc32c-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.1/240.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.6/156.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading google_crc32c-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.7/221.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.68.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.68.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyasn1, protobuf, grpcio, google-crc32c, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.3\n",
      "    Uninstalling protobuf-4.24.3:\n",
      "      Successfully uninstalled protobuf-4.24.3\n",
      "Successfully installed cachetools-5.5.0 google-api-core-2.23.0 google-auth-2.36.0 google-cloud-bigquery-3.27.0 google-cloud-core-2.4.1 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.66.0 grpcio-1.68.0 grpcio-status-1.68.0 proto-plus-1.25.0 protobuf-5.28.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5b7aeed-5f4b-448f-8dc8-e6307a48fc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully uploaded to BigQuery.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# upload to BigQuery\n",
    "client = bigquery.Client(project=\"chromatic-pride-435508-i5\")   \n",
    "\n",
    "dataset_id = \"chromatic-pride-435508-i5.flights\"\n",
    "table_id = f\"{dataset_id}.Delay_Pipeline\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1 # ignore the header\n",
    "job_config.autodetect = True\n",
    "\n",
    "\n",
    "gcs_path = \"gs://de_as2_data/joined_flights_delaydata_final/*.csv\"  # *.csv combines all files to result in a whole dataframe\n",
    "\n",
    "load_job = client.load_table_from_uri(gcs_path, table_id, job_config=job_config)\n",
    "\n",
    "# Wait for the job to complete\n",
    "load_job.result()\n",
    "print(\"Data successfully uploaded to BigQuery.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c5668-ddb8-4f60-9528-6e9b5e83a118",
   "metadata": {},
   "source": [
    "### Stop spark session ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2aaed49-d47e-4d2e-be9c-0284363fa43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e9a06-33f3-4d3f-b1b6-78fb48bbe4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
